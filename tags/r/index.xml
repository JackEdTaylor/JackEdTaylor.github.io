<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Jack E Taylor</title>
    <link>/tags/r/</link>
    <description>Recent content in R on Jack E Taylor</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <lastBuildDate>Mon, 22 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LexOPS</title>
      <link>/project/01-lexops/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/project/01-lexops/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Item-Wise and Distribution-Wise Matching</title>
      <link>/2020/06/22/item-wise-and-distribution-wise-matching/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/22/item-wise-and-distribution-wise-matching/</guid>
      <description>I’m often surprised just how many papers in Psycholinguistics (and elsewhere I’m sure) give such little detail about their stimuli. My pet hate is when papers try to show that stimuli are controlled by reporting non-significant p values from t-tests or ANOVAs on the differences in confounds between conditions. This is a problem because:
You’re assuming normality in the distributions. Especially for small samples of stimuli, this is unlikely to be the case.</description>
    </item>
    
  </channel>
</rss>